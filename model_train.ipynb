{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model_train.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS2YBPeW5uVN",
        "outputId": "9de7c8f8-b96f-41a9-ce49-73c1f2ee539e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 100277, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (31/31), done.\u001b[K\n",
            "remote: Total 100277 (delta 16), reused 29 (delta 12), pack-reused 100231\u001b[K\n",
            "Receiving objects: 100% (100277/100277), 85.40 MiB | 23.36 MiB/s, done.\n",
            "Resolving deltas: 100% (72807/72807), done.\n",
            "Processing /content/transformers\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 14.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0.dev0) (4.10.1)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 55.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0.dev0) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0.dev0) (1.19.5)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 67.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0.dev0) (3.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.17.0.dev0) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.17.0.dev0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.17.0.dev0) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.17.0.dev0) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.17.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.17.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.17.0.dev0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.17.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.17.0.dev0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.17.0.dev0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.17.0.dev0) (7.1.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.17.0.dev0-py3-none-any.whl size=3649365 sha256=8edfbe721161d32f80c1ef29be077e022985e9b159990d2758a6883c1ab343bd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-y7ocj3kr/wheels/49/62/f4/6730819eed4e6468662b1519bf3bf46419b2335990c77f8767\n",
            "Successfully built transformers\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.4.0 pyyaml-6.0 sacremoses-0.0.47 tokenizers-0.11.4 transformers-4.17.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers;\n",
        "!cd transformers; pip3 install .\n",
        "#The distilled gpt-2 model comes from the huggingface repository using the transformers library. This section of the code imports transformers."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!bash run_experiments.sh\n",
        "#The following bash file trains the model over 4 epochs and makes checkpoints every 500 steps."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_InR5oP6JJ3",
        "outputId": "dbf15160-68d5-4e8a-9ac0-8e35f8f6ca47"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "02/09/2022 19:19:59 - WARNING - __main__ -   Process rank: -1, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False\n",
            "02/09/2022 19:19:59 - INFO - __main__ -   Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=IntervalStrategy.NO,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_model_id=None,\n",
            "hub_strategy=HubStrategy.EVERY_SAVE,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=-1,\n",
            "log_level=-1,\n",
            "log_level_replica=-1,\n",
            "log_on_each_node=True,\n",
            "logging_dir=experiments/epochs_4/runs/Feb09_19-19-59_4e7917946a09,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=IntervalStrategy.STEPS,\n",
            "lr_scheduler_type=SchedulerType.LINEAR,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=4.0,\n",
            "optim=OptimizerNames.ADAMW_HF,\n",
            "output_dir=experiments/epochs_4,\n",
            "overwrite_output_dir=True,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=2,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=experiments/epochs_4,\n",
            "save_on_each_node=False,\n",
            "save_steps=500,\n",
            "save_strategy=IntervalStrategy.STEPS,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "tf32=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_legacy_prediction_loop=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            "xpu_backend=None,\n",
            ")\n",
            "Downloading: 100% 762/762 [00:00<00:00, 914kB/s]\n",
            "Downloading: 100% 0.99M/0.99M [00:00<00:00, 2.06MB/s]\n",
            "Downloading: 100% 446k/446k [00:00<00:00, 1.08MB/s]\n",
            "Downloading: 100% 1.29M/1.29M [00:00<00:00, 3.15MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/auto/modeling_auto.py:841: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
            "  FutureWarning,\n",
            "Downloading: 100% 336M/336M [00:06<00:00, 57.6MB/s]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:58: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/master/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (5265981 > 1024). Running this sequence through the model will result in indexing errors\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/trainer.py:1135: FutureWarning: `model_path` is deprecated and will be removed in a future version. Use `resume_from_checkpoint` instead.\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 5142\n",
            "  Num Epochs = 4\n",
            "  Instantaneous batch size per device = 2\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 10284\n",
            "{'loss': 3.0973, 'learning_rate': 4.756903928432517e-05, 'epoch': 0.19}\n",
            "  5% 500/10284 [03:39<1:12:32,  2.25it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-500\n",
            "Configuration saved in experiments/epochs_4/checkpoint-500/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-500/pytorch_model.bin\n",
            "{'loss': 2.9337, 'learning_rate': 4.513807856865033e-05, 'epoch': 0.39}\n",
            " 10% 1000/10284 [07:26<1:09:07,  2.24it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-1000\n",
            "Configuration saved in experiments/epochs_4/checkpoint-1000/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-1000/pytorch_model.bin\n",
            "{'loss': 2.8436, 'learning_rate': 4.2707117852975496e-05, 'epoch': 0.58}\n",
            " 15% 1500/10284 [11:12<1:05:12,  2.24it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-1500\n",
            "Configuration saved in experiments/epochs_4/checkpoint-1500/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-1500/pytorch_model.bin\n",
            "{'loss': 2.803, 'learning_rate': 4.027615713730066e-05, 'epoch': 0.78}\n",
            " 19% 2000/10284 [14:58<1:01:40,  2.24it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-2000\n",
            "Configuration saved in experiments/epochs_4/checkpoint-2000/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-2000/pytorch_model.bin\n",
            "{'loss': 2.7627, 'learning_rate': 3.7845196421625825e-05, 'epoch': 0.97}\n",
            " 24% 2500/10284 [18:45<57:49,  2.24it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-2500\n",
            "Configuration saved in experiments/epochs_4/checkpoint-2500/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-2500/pytorch_model.bin\n",
            "{'loss': 2.6962, 'learning_rate': 3.541423570595099e-05, 'epoch': 1.17}\n",
            " 29% 3000/10284 [22:31<54:16,  2.24it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-3000\n",
            "Configuration saved in experiments/epochs_4/checkpoint-3000/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-3000/pytorch_model.bin\n",
            "{'loss': 2.6611, 'learning_rate': 3.298327499027616e-05, 'epoch': 1.36}\n",
            " 34% 3500/10284 [26:17<50:37,  2.23it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-3500\n",
            "Configuration saved in experiments/epochs_4/checkpoint-3500/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-3500/pytorch_model.bin\n",
            "{'loss': 2.6403, 'learning_rate': 3.0552314274601326e-05, 'epoch': 1.56}\n",
            " 39% 4000/10284 [30:04<46:48,  2.24it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-4000\n",
            "Configuration saved in experiments/epochs_4/checkpoint-4000/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-4000/pytorch_model.bin\n",
            "{'loss': 2.6366, 'learning_rate': 2.8121353558926487e-05, 'epoch': 1.75}\n",
            " 44% 4500/10284 [33:50<43:06,  2.24it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-4500\n",
            "Configuration saved in experiments/epochs_4/checkpoint-4500/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-4500/pytorch_model.bin\n",
            "{'loss': 2.6278, 'learning_rate': 2.5690392843251655e-05, 'epoch': 1.94}\n",
            " 49% 5000/10284 [37:37<39:21,  2.24it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-5000\n",
            "Configuration saved in experiments/epochs_4/checkpoint-5000/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-5000/pytorch_model.bin\n",
            "{'loss': 2.5768, 'learning_rate': 2.325943212757682e-05, 'epoch': 2.14}\n",
            " 53% 5500/10284 [41:24<35:36,  2.24it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-5500\n",
            "Configuration saved in experiments/epochs_4/checkpoint-5500/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-5500/pytorch_model.bin\n",
            "{'loss': 2.5523, 'learning_rate': 2.0828471411901985e-05, 'epoch': 2.33}\n",
            " 58% 6000/10284 [45:10<31:50,  2.24it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-6000\n",
            "Configuration saved in experiments/epochs_4/checkpoint-6000/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-6000/pytorch_model.bin\n",
            "{'loss': 2.5594, 'learning_rate': 1.839751069622715e-05, 'epoch': 2.53}\n",
            " 63% 6500/10284 [48:57<28:01,  2.25it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-6500\n",
            "Configuration saved in experiments/epochs_4/checkpoint-6500/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-6500/pytorch_model.bin\n",
            "{'loss': 2.5617, 'learning_rate': 1.5966549980552314e-05, 'epoch': 2.72}\n",
            " 68% 7000/10284 [52:43<24:26,  2.24it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-7000\n",
            "Configuration saved in experiments/epochs_4/checkpoint-7000/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-7000/pytorch_model.bin\n",
            "{'loss': 2.5567, 'learning_rate': 1.353558926487748e-05, 'epoch': 2.92}\n",
            " 73% 7500/10284 [56:30<20:46,  2.23it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-7500\n",
            "Configuration saved in experiments/epochs_4/checkpoint-7500/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-7500/pytorch_model.bin\n",
            "{'loss': 2.529, 'learning_rate': 1.1104628549202645e-05, 'epoch': 3.11}\n",
            " 78% 8000/10284 [1:00:16<17:00,  2.24it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-8000\n",
            "Configuration saved in experiments/epochs_4/checkpoint-8000/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-8000/pytorch_model.bin\n",
            "{'loss': 2.506, 'learning_rate': 8.673667833527811e-06, 'epoch': 3.31}\n",
            " 83% 8500/10284 [1:04:03<13:11,  2.25it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-8500\n",
            "Configuration saved in experiments/epochs_4/checkpoint-8500/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-8500/pytorch_model.bin\n",
            "{'loss': 2.519, 'learning_rate': 6.2427071178529756e-06, 'epoch': 3.5}\n",
            " 88% 9000/10284 [1:07:50<09:32,  2.24it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-9000\n",
            "Configuration saved in experiments/epochs_4/checkpoint-9000/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-9000/pytorch_model.bin\n",
            "{'loss': 2.5221, 'learning_rate': 3.811746402178141e-06, 'epoch': 3.7}\n",
            " 92% 9500/10284 [1:11:36<05:47,  2.25it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-9500\n",
            "Configuration saved in experiments/epochs_4/checkpoint-9500/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-9500/pytorch_model.bin\n",
            "{'loss': 2.5092, 'learning_rate': 1.3807856865033063e-06, 'epoch': 3.89}\n",
            " 97% 10000/10284 [1:15:23<02:06,  2.24it/s]Saving model checkpoint to experiments/epochs_4/checkpoint-10000\n",
            "Configuration saved in experiments/epochs_4/checkpoint-10000/config.json\n",
            "Model weights saved in experiments/epochs_4/checkpoint-10000/pytorch_model.bin\n",
            "100% 10284/10284 [1:17:33<00:00,  2.24it/s]\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 4653.5008, 'train_samples_per_second': 4.42, 'train_steps_per_second': 2.21, 'train_loss': 2.6505279006868983, 'epoch': 4.0}\n",
            "100% 10284/10284 [1:17:33<00:00,  2.21it/s]\n",
            "Saving model checkpoint to experiments/epochs_4\n",
            "Configuration saved in experiments/epochs_4/config.json\n",
            "Model weights saved in experiments/epochs_4/pytorch_model.bin\n",
            "Traceback (most recent call last):\n",
            "  File \"run_lm_finetuning.py\", line 308, in <module>\n",
            "    main()\n",
            "  File \"run_lm_finetuning.py\", line 276, in main\n",
            "    if trainer.is_world_master():\n",
            "AttributeError: 'Trainer' object has no attribute 'is_world_master'\n"
          ]
        }
      ]
    }
  ]
}